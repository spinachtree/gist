title	The Story of Gist

sect	Introduction

This is the story behind Gist and how it evolved. If you just want to find out what Gist does and how to use it, then skip this note and jump across to: @[http://spinachtree.org/gist/guide.html]

Traditional grammar rules are concise and powerful, and they should be fun to use in a software application. So why do they almost always end up on the side-lines? Even when they are used in the formal specifications they get left behind in those documents. All too often different software products that claim to implement the same grammar specification end up with slightly different versions that create annoying quirks and defects.

The grammar rules look like software instructions, so why can't we just automate them? There are text books full of various techniques to implement a parser for a grammar, but it turns out to be harder than it looks to fully automate a grammar. Good parser generator tools are available, but they are heavy duty tools that are mostly used by specialist designers for a major application such as a new programming language. Many small grammar specifications do not seem to warrant these heavy duty tools, and they look easy enough to implement directly in an ad-hoc program, and that's what leads to the annoying quirks and defects.

The syntax for a programming language is almost always specified with a grammar. But grammar rules work just as well for simpler specifications such as data records, message formats and many other applications. The IETF Internet standards have used grammar specifications since the 1970s. Working with some of these grammars I found that they could be expressed with a simplified form of grammar rules that could be automated. This makes a grammar easy to design and develop and easy to use in an application program, it is fun, and it was the starting point for the Gist project. During the development of new specifications automated grammar rules can be easily modified and tested.

This experience established that at least some simple grammars could be fully automated. Grammar rules can be run as if they are computer instructions. Sometimes it required a little re-write of the standard grammar rules, but nothing major. Only later did it emerge that almost any practical grammar could be automated.

For many years I had assumed that programming languages might evolve to support grammar rules directly. But this has not yet happened, although regular expressions are now in common use, and several programming languages do support them directly. There are a few exceptions: the logic programming language Prolog supports grammar rules, and the functional language Haskell has parser support which is now available in Scala. There are also very good parser generator tools, such as the #ANTLR software, but this is not a fully automated process.

Perhaps part of the problem is that grammar rules have been too successful! The first BNF grammar rule form was published in 1959, and the form used today remains much the same. There is a well established body of formal theory, and the design of parsers is a standard part of computer science text books. Compiler writers have T-shirts saying they do it bottom up, and the text books are full of various bottom-up table driven parsers. The #LEX and #YACC compiler generator tools first appeared in 1975 and they have had great success ever since. Unfortunately the recipe has become entrenched dogma, complete with some folk myths that should be debunked.

It is a myth that a context-free grammar is always the best choice, it is a myth that a bottom-up parser is always better than a top-down parser, and it is a myth that a top-down parser can not handle left recursive rules. Its also a myth that a parser needs a front-end lexical scanner to provide tokens for the back-end grammar rule parser (as in LEX/YACC). This structure is efficient, but it adds a complexity that is hard to fully automate. More flexible grammar rules can specify the syntax down to the character level and eliminate a front-end scanner.


sect	Simple One-Match Rules

A simple interpretation of grammar rules as pattern matching templates is not a full implementation of context-free grammar rules. But it turns out to be a very effective way to implement practical computer grammars. In fact it seems that all practical grammars can be expressed in a way that is simple to interpret.

A context-free grammar has expressive power that is just not needed for computer applications. A context-free grammar can define an ambiguous language, and natural languages can be ambiguous, but a computer language avoids ambiguity like the plague. A much simpler grammar language can be used for unambiguous computer applications.

Over the last ten years I have implemented simplified grammar rules for a variety of grammars in several different programming languages. The simple rules did not implement a context-free grammar, but they always worked perfectly well for the applications I was interested in. 

The key concept was to interpret grammar rules as "one-match" pattern matching expressions. In particular, a rule can match a string in only one way, if a rule could match the input in more than one way then the "winning" choice is prescribed. The simple grammar rules eliminate ambiguity. Traditional grammar rules are "multi-match", and they usually match the longest string first, but they reserve the option to back-track later and match shorter prefix strings.

At first it seemed that one-match rules would be too simple, and they would limit the expressive power of the grammar. But that just didn't happen, in practice grammars didn't need multi-match rules, and when a grammar did employ a multi-match rule it could be re-formulated with one-match rules. Grammars that can not be expressed with one-match rules do exist, but they always seem to be contrived text-book examples.  

In fact one-match rules are often much better than traditional multi-match rules since they prevent the rules matching undesired patterns that can be just plain wrong. For example, an integer may be specified as a sequence of one or more digit. The intention is to match the ~longest string of digits, but a traditional grammar rule has no way to say that, it always reserves the option to match a shorter prefix integer. This allows the trailing digits to be matched by some subsequent rule, and the only way to meet the design intent is to ensure that there are no other rules that can match these digits.

Not only is there no way to express the fact that ~only the longest match is valid, but an automated parser has to work hard to figure out that shorter matches will never work. A simple context-free grammar parser may need to back-track and try shorter and shorted prefixes just in case a subsequent rule will start to match. Building a fast parser gets complicated. 


sect	Parser Expression Grammars

In 2004 Bryan Ford published papers on his #PEG (Parser Expression Grammar) work. This was good news, a formal basis for a simpler grammar with one-match rules. It turns out that there are languages that a PEG can express but a traditional CFG (Context Free Grammar) can not, but others that a CFG can express but a PEG can not. But practical CFG languages can almost always be specified with a simpler PEG.

A PEG is not only simpler to parse it also enables an efficient pack-rat parser which can parse in linear time. This work has revived interest in new work on parser design, an area that had for many years been considered exhausted.  

The PEG rules are "one-match" rules, since they are based on a sequential first match choice, "x / y", rather than the more general CFG choice. In Gist I had been using a slightly different choice operator that matched the ~longest successful match, "x | y" (as a one-match rule).

The longest match has the advantage that the choices can be expressed in any order, as in a traditional grammar. The first sequential choice is more like a programming language and the order does matter, but it can be more efficient since after the first match any subsequent choices can be ignored.  

The longest match is slightly more general and it can express a first sequential choice with the help of negation:

eg
	x / y	=>	x | !x y

If only one choice operator is desired it is debatable which one is best. The longest match is more general and may be easier to use, but the first choice operator can be more efficient and seems to work well in practice. It can also be argued that using the "x | y" syntax for the longest match is confusing since it is not quite the same as the traditional "multi-match" operator. I have tried one and the other and both, and I can't see much in it. I moved to a first choice operator in line with a PEG.

The problem for people who are familiar with multi-choice rules is more likely to be with the repeat operators than with the choice operator itself. The * repeat operator in a one-match rule will match as many times as possible, but it will not back-off to allow trailing elements to be matched by another term. Any rules that assume a repeat operator will back-off must be re-formulated in some other way, which is always possible, and often improves the grammar.


sect	Grammar Implicit Syntax Tree

My objective for Gist was to provide a grammar language with a universal parser so that a grammar specification could be directly automated. A PEG like grammar that enables a simple and efficient parser is a big step forward, but to automate the parser we need to specify the parser tree output structure.

Traditional grammar rules do not specify the parse tree structure, different parsers can resolve the grammar rules into different parse tree structures. But to automate a grammar the exact way that the grammar rules generate the parse tree must be implicit in the grammar.

Although the Gist grammar language can be defined independent of a programming language, the universal parser to automate it must be implemented in some particular programming language(s). Application software then requires a programming language interface to be able to use the parse tree generated by a Gist parser. The complexity of the XML DOM interface to do this for XML does not bode well. The Gist approach is to keep the parse tree implementation bare bones simple and efficient. The simple parse tree can be used directly for simple applications, but more complex applications may well transform the parse tree into an application specific tree structure that best suits the particular implementation.

When a parser is hand crafted, or built with a parser generator tool, it is built for a specific grammar, and the parse tree and application interface can be tailored to the specific grammar's application and implementation requirements. In particular an application may integrate semantic methods into the grammar rules, and these methods are programming language specific. A universal parser must provide a single simple interface that is acceptable to a wide range of grammar applications and implementations.


sect	Grammar Specifications as Programs

There is often a significant difference between a specification and its implementation. A good specification must be precise, and free of any specific implementation details.

A specification will often have a declarative reading, but not necessarily a procedural reading that can be automated. For example, a sorted list can be easily specified, but that does not mean it can be automated as a universal sorting algorithm. An ideal solution has both a declarative and procedural reading, as in a logic-programming language. Fortunately it is not too difficult to ensure that the Gist grammar rules work this way.

There are two styles of parser design, one that processes the complete input string and generates a parse tree for the application, and another that generates a stream of events as the parser matches elements in the input string. The full parse tree style is usually simpler for an application to deal with, but an event style may be needed for processing a very large input string or a continuous input stream.

A Gist grammar inherently defines a parse tree structure, but the rules can include event elements to enable an event driven parser. As a specification the Gist grammar should avoid event, but an implementation can choose to add event elements for an event driven style.

In practice formal grammar specifications are sometimes not quite complete, and natural language is used to cover the holes in the grammar. For example, the specification may require a feature that can not be expressed as a context-free grammar, or maybe it is just too awkward to express. For an automated grammar there needs to be some way to express arbitrary features, and to allow an escape into a programming language implementation if necessary. In Gist the event action interface provides an escape-hatch for any special requirements.

The performance of an automated grammar needs to be reasonably competitive when compared to a dedicated parser program for the same grammar. Of course a fully automated parser eliminates custom design of the parse tree, fine tuning of parser machinery, and close integration with an application program. For ultimate performance and application integration you may need to move to a custom crafted parser for your particular grammar. The #ANTLR parser generator project provides great tools and documentation to enable you to do that. Terence Parr's book #[Language Implementation Patterns] explains many of the design issues involved in crafting parsers and interpreters.


sect	Conclusions

After years of fun playing around with grammar rule formats and experimenting with different automated parsers the Gist grammar language has evolved into a useful tool, and I have convinced myself that:

*	Grammar rules can be fun.

*	A simple Gist PEG-like interpretation is practical and effective.

*	Formal grammar specifications could easily adopt Gist rules.

*	Gist can automate grammar verification testing and demonstration.

*	A Gist universal parser is quite practical for many applications.

*	A Gist grammar specification remains open to any other implementation.


sect	References

refs
	ANTLR
		Another Tool for Language recognition. @[http://www.antlr.org/]
	Language Implementation Patterns
		Terence Parr, Create Your Own Domain-Specific and General Programming Languages, ISBN: 978-1-93435-645-6
		@[http://pragprog.com/titles/tpdsl/language-implementation-patterns]
	LEX
		Lexical token scanner generator, @[http://en.wikipedia.org/wiki/Lex_(software)]
	PEG
		Parser Expression Grammar: @[http://pdos.csail.mit.edu/~baford/packrat/thesis/]
	YACC
		Yet Another Compiler Compiler, @[http://en.wikipedia.org/wiki/Yacc]

format
	title	h1:prose
	sect	h2:prose
	sub1	h3:prose
	para	p:prose
	eg	pre:text
	refs	dl:format
	*	ul:format

prose
	:_	tt
	~_	em
	"_"	quote
	#_	linkID
	@[_]	link

style
	.undef { color:red; background-color:yellow; }
	.title, .author, .date { text-align: center; }
	.eg { background-color:#FFFDDA; border: thin solid #ECECEC; }
	dl { font-size:small; }
	dl dd p { margin: 0pt 0pt 10pt 0pt; }
	.format, .prose, .style { display:none; }



