
title	The Lazy Cashin Parser

sect	Introduction

A top down recursive descent parser is a simple and elegant way to parse grammar rules. Unfortunately the price for this simplicity has often been the inability to handle left recursive rules, and the possibility that a grammar may require an exponential parse time #Parsing.

A cache memory for parse results can be used to enable a recursive descent parser to handle left recursion, and also to remove the possibility of an exponential time explosion. For a Parser Expression Grammar, #PEG, a packrat parser, can achieve a linear parse time performance.

Unfortunately the addition of a full packrat cache can add an overhead without much advantage to many grammars, #Memos.  Also, the use of cache memos to gain performance conflicts with the more limited use of a grammar rule memo for left recursion.

Dealing with left recursive rules requires a memo of a previous result which can act as a "seed" to grow a larger result. But it does not require a memo of every rule result for each input start position, as used in a packrat parser. It is sufficient to have a single memo for a grammar rule that can be updated each time the rule is applied.

The Lazy Cashin Parser is a recursive descent parser with a lazy cache memo that avoids cache overhead whenever possible. If a grammar is well behaved the parser will incur almost no extra overhead. Left recursion is implemented with a very simple light-weight single memo per rule. A badly behaved grammar rule can be detected and a full packrat memo scheme may then be used for that rule.


sect	Left Recursion

sub1	The Problem

A simple top down parser will fall into a endless loop with a left recursive rule such as this:

eg
	exp = exp '-' int / int
	int : digit+

The desired parse tree result for :["5-3-2"] is :[(((5)-3)-2)]:

eg
	exp
		exp
			exp	int	"5"
			int	"3"
		int	"2"

A top down parse can be explained with pencil and paper along these lines: To parse the :exp rule we must parse its body, which has a choice: :[(exp '-' int / int)]. The first choice starts with a recursion, if we had a solution for matching a :exp we could build on it, but without a known solution we only have the second choice, an :int which will match ":[5]". We now have a first ~seed solution for :exp matching the input ":[5]".

Given a solution we can go back to the first choice and substitute this first :exp match result to generate a second solution ":[5-3]". The new solution has expanded the first seed result, and we can substitute once more to match ":[5-3-2]". It is an implicit assumption that we want to find the longest possible parse, so we should continue this process as many times as we can. In this case we are done, the solution matches ":[5-3-2]".

So the problem is how to implement this process in a top down parser.


sub1	An Implementation

The basic strategy to parse left recursion is:

*	Detect a loop, and fail (e.g. the :exp rule calling itself).

*	Record a seed result for the rule (e.g. the other choice, :[int "5"])

*	Repeat a parse of the rule to grow the seed result.

A loop is detected when a rule is invoked for a second time at the same input scan position but before it has generated a first result. When a loop is detected it will be a nested recursive call, and by returning a fail result the outer call will try to find another choice option to match the rule.

It is simple to detect a loop by recording a copy of the dynamic local variables for the start position and the result, in the context of the rule, outside the dynamic call stack. The start and result variables may be implemented in a global context indexed by the rule name, or if the rule is an instance of an object then the start and result values are simply instance variables:

eg
	parser	-- global context for the parse
		input	-- string being parsed
		cursor	-- current index position
		rule	-- context for an instance of a rule
			start=-1	-- records cursor from last parse of this rule
			result=null	-- records the result from last parse of this rule
			parse	-- dynamic context ie local variables
				index = cursor	-- new rule.parse start position
				if (index==start and result==null) return false -- loop detected...
				start=index	-- new start for this rule
				result=null	-- not a valid parse result value
				result=rule.body.parse	-- may contain recursive rule.parse calls
				return result

In general a memo recording a parse result needs to be indexed by the rule and the cursor start position. Inside a rule instance a memo can be indexed by a start cursor value alone. Here we are only recording single values for the start and result and updating them each time the rule is used to parse the input. In other words a single memo of the latest parse.

The first result after a recursive call has been failed becomes a seed result for the rule, and a repeat loop is then used to grow the seed. Each time the rule is invoked in the repeat loop it will return the previous result (it acts as a memo), and the parse will grow this seed into a longer match if possible. If the new result is a longer match then it replaces the previous result, otherwise we are done.

eg
	parse	-- global context for the parse
		input	-- string being parsed
		cursor	-- current index position
		rule	-- context for an instance of a rule ie object instance variables
			start	-- records cursor for this rule
			result	-- records the result from last parse of this rule
			loop	-- flag if this rule detects left recursion
			parse	-- dynamic context ie local method variables
				index = cursor	-- new rule.parse start position
				if (index==start)  -- deja vu
					if (result==null)
						loop=true	-- left recursion detected
						return false	-- parse fails...
					else if (loop) -- only use memo for left recursion
						return result	-- seed grows to final result
				start=index
				result=null	-- not a valid parse result
				seed=rule.body.parse	-- may contain recursive rule.parse calls
				if (loop)
					repeat  -- grow the seed result
						cursor=index  -- reset input cursor
						start=index
						result=seed
						grow=rule.body.parse
						if (grow =< seed) break
						seed=grow
				result=seed
				return result

This solution is not difficult to implement, and if the rules are implemented as objects with a parse method then the program code is little more than the outline sketch shown above. Notice that we have a memo result for the last match of every rule, so it could be used to speed up a rule that needs to be re-parsed after a failure backtrack. This would give a light-weight form of packrat parser, but it also pulls the rug out from under the left recursion, as we will see. After looking at left recursion we will return to this issue to see to how to also use memos for speed up.

A parser also needs to build a parse tree, and each successful match result represents a new parse tree node. When a seed result is returned the corresponding node needs to be added into the parse tree, but no new machinery is required. Growing the seed generates a node in the parse tree as usual, and when the repeat loop needs to reset the parse tree and the input scan it uses the same mechanism as a failure backtrack. The machinery to build the parse tree therefore remains essentially unchanged by this implementation of left recursion, and the overhead on all other rules is insignificant.


sub1	Harder Problems

The problem we started with is a simple left recursion, but the solution can handle harder problems than that. For example an indirect recursion:

eg
	Exp   = Sub / int
	Sub   = Exp '-' int
	int   : digit+

The parse tree for input :["5-3-2"] should be :[(((5)-3)-2)]:

eg
	Exp	Sub
		Exp	Sub
			Exp	int	"5"
			int	"3"
		int	"2"

The :Exp rule is indirectly left recursive via the transit :Sub rule. This is a simple indirect loop, in general there may be an arbitrary number of transit rules that end up looping back to call the first rule again.

This is why we are only using a memo for a rule that has detected a left recursive loop (for now). If a memo is automatically used to speed-up all rules then the implementation would fail to handle indirect recursion. For example, invoking the :Exp rule results in the first attempt to match the "Sub" rule returning a failure since the :Exp rule detects a loop. If the :Sub rule recorded a memo then we would not be able to invoke the :Exp rule again in order to build a longer match on top of the first "seed".

Another complication is the possibility that a left recursive loop may contain another nested left recursion. For example:

eg
	Exp   = Add | Sub | Term
	Add   = Exp '+' Term
	Sub   = Exp '-' Term
	Term  = Mul | Div | Val
	Mul   = Term '*' Val
	Div   = Term '/' Val
	Val   = int | '(' Exp ')'
	int   : '0'..'9'+

A parser for this grammar should parse an input such as :["1+2*(3-4/2+1)"] into the tree:

eg
	Exp	Add
		Exp	Term	Val	int	"1"
		Term	Mul
			Term	Val	int	"2"
			Val	Exp	Add
				Exp	Sub
					Exp	Term	Val	int	"3"
					Term	Di
						Term	Val	int	"4"
						Val	int	"2"
				Term	Val	int	"1"

The implementation sketched out above can handle this grammar, but in general the repeat loop should be properly re-entrant. This can be done with a stack to record the index and result that the loop is working on:

eg
	if (loop)
		stack.push(index,seed)
		repeat  -- grow the seed result
			cursor=index  -- reset input cursor
			start=index
			result=seed
			grow=rule.body.parse
			if (grow =< seed) break
			seed=grow
			(index,seed)=stack.top()
		result=seed
		(index,seed)=stack.pop()

An implementation need only create a stack if it is needed, so the extra overhead on left recursion is small, and there is no extra overhead on rules that are not left recursive.


sub1	Even Harder Problems

There are some even harder problems that this simple implementation does not fully deal with, but they are rather contrived examples and do not seem to be a big issue in practice.

First, if the rule has both left and right recursion then the parse tree will not be balanced, the right recursion will match more than the left recursion, for example:

eg
	Add   = Exp '+' Exp

A rule like this is inherently ambiguous in a traditional grammar, and there does not appear to be any practical need to write this kind of rule. A parser using the implementation discussed here will generate an unbalanced parse tree, with preference for the right recursion.

Another problem arises with a left-left recursion, where a left recursion calls itself again. This happens when a rule is both direct and indirectly left recursive, for example:

eg
	A = A a / B
	B = B b / A / C
	C = C c / B / d

The left-left recursions are pruned by the parser, so this grammar will be parsed as:

eg
	A = A a / B  -- definition
	  = B a*     -- A recursion
	  = (B b / A / C) a* -- B rule
	  = (A / C) b* a*    -- B recursion
	  = C b* a*          -- A left-left recursion pruned
	  = (C c / B / d) b* c*  -- C rule
	  = (C c / d) b* c*  -- B left-left recursion pruned
	  = d c* b* a*       -- C recursion

The pruned loops mean that these rules will not match some strings that a full expansion would match. Without pruning these rules would match: :[d (c | b | a)*].

Nested left recursions are seen in practical grammars, but left-left recursion is very unusual, and not seen in practice, so pruning left-left loops does not seem to be an unreasonable restriction.


sect	Packrat Cache

A packrat cache may be employed to enhance performance. It records a memo for every result at every input position. But in practice many grammars will never use a rule to match the same position more than once, and if this does happen a single memo of the last match is often sufficient.

The light-weight memos, as used for left recursion, can be used as a partial substitute for a packrat cache. They essentially come for free, but as we have seen it can be fatal to employ a memo inside an indirect left recursion.

It is ironic that the use of packrat memos has inspired the design of extensions to handle left recursion, since making left recursion work properly in a full packrat parser is more complicated than using a memo for left recursion alone. A detailed discussion of handling left recursion in a packrat parser can be found in #LRPP.  The design described here is quite a bit simpler.

Another problem with using memos is the possibility that a grammar rule may not have a ~fixed result, in other words it may match different results from the same input position. The one-match committed choice rules used in a PEG grammar always generate a fixed result. But a context-sensitive operator or extensions implemented in a programming language (such as an :[<event>] in #Gist) do not necessarily generate fixed results. A left recursive rule is not a fixed rule since the result changes as it is built up from an initial seed result.

We can determine if a rule generates a fixed result by building rules up from the bottom. One-match literal character terminal rules will be fixed, and composite rules will be fixed if they only refer to other rules that are known to be fixed, and only contain fixed result operators. A reference to a rule that is not fixed will contaminate all higher level rules.

The analysis of rule dependencies is not too difficult, and it is a natural part of compiling the grammar rules to optimize the performance of the rules. Rule dependencies will reveal loops, and a rule that contains a reference to an unresolved rule is ~not marked as fixed. The interesting thing about this process is that this will automatically identify the transit rules in a left recursive loop.

Rules that are called inside a loop can be marked as fixed if they only contain references to fixed rules. A grammar where all terminal rules are fixed will then be able to mark composite rules as fixed, except for rules that contain a loop (left or right recursive).

In other words, the rules are mapped into a acyclic graph. A cycle in the rule graph is broken by a leaf node that will ~not be marked as fixed. Other nodes are marked as fixed unless they are context sensitive operators, and provided all the node in their sub-tree are fixed. A memo will not be used to short-circuit a parse unless the rule is fixed.

The Lazy Cashin Parser uses a light-weight memo for left recursion and also uses these memos to gain performance on other rules provided they are marked as fixed. This means that there is almost no overhead, but a cache memo is available to boost the performance for most back-track failures.

If a rule starts to match at an input position that is prior to the start position of the current result memo, then the light-weight memo will be lost. This may result in a badly performing rule, but it seems to be quite a rare occurrence. When it does occur then it can be detected, and this could trigger the parser to allocate a memo table for this rule. From then on this particular rule could use a full packrat memo cache. It is not clear if this is that important in practice.


sect	Building The Parse Tree

A recursive descent parser can also use a lazy process for building the parse tree by delaying the creation of a node object until the end of a rule being parsed. This allows the parser to explore options that fail with minimal overhead. Creating an object may be a very low cost operation, but a grammar can frequently explore many different options that fail.

A disadvantage of a lazy process is that the current nesting of the rules being matched is hidden in the main procedure call evaluation stack, and is not easily available in the parser. This is not an issue in most cases, but it is a limitation for grammar debug tracing, and it may be required to handle left recursion in a full packrat parser. The left recursion method discussed here avoids this because it has no run-time need to identify transit rules that are called inside a recursive loop.

The #Gist grammar can use an event for a debug trace. The grammar rule that an event occurs in is not easily known at run-time, but it is known at compile time and can be provided to the event parse operator.

The parse tree itself needs very little structure. A single linked list of nodes can be used, so failure backtracking is very low cost (just reset the pointer tracking the last node in the list). When a new node is created the list of nodes generated after the point the rule was invoked are linked into the child link of the new node. The new node replaces its children in the parsers list of nodes.

The Gist parser also maintains a reverse list link pointer to allow prior nodes to be accessed. This is only used for a context-sensitive operator. A reverse link also allows an application to find a parent node in a parse tree, although that is not usually required.

sect	Performance

The performance of a parser is often dominated by low level character matching operations and many practical grammars can be parsed in linear time without memos. But even a simple grammar can involve matching complex character sets with multiple code point ranges, particularly with Unicode characters.

The challenge for a parser is to compete with regular expression matchers. Grammar rules that match a leaf node in the parse tree may employ the same techniques used for regular expressions. But the rules may express a character set as the composition of other character sets that best express the design intent. A simple compiler can help reduce many of these expressions to composite character sets.


sect	Conclusion

The implementation of left recursion in a top down parser can be remarkably simple, yet it can cope with complex indirect and nested left recursive rules. There is also very little overhead added to the basic top down recursive descent parser, a couple of variables and a simple test are the only burden.

The light-weight memo used for left recursion may also be used to avoid the need to re-parse a rule from the same start position. But it is not always safe to use a memo, we must be sure that the rule is a pure function with a fixed result that is always identical. A left recursive rule is not fixed, since it grows with each application. A static analysis can identify rules with a fixed result, and a rule can only be marked as fixed if all the rules it depends on are fixed, which excludes loops.

This note is about program implementation, for more analysis consider ~fixed parse operators as ~regular functions #Stepanov, and the lazy memo scheme as a form of dynamic programming #Skiena.

Finally, an apology for the pun on my name and my nature, it's not often that being so lazy works so well.


sect	References

refs
	Gist
		A Grammar Language implemented with a Lazy Cashin Parser, see: @[http://spinachtree.org/gist]
	Parsing
		@[http://en.wikipedia.org/wiki/Parsing]
	PEG
		@[http://en.wikipedia.org/wiki/Parsing_expression_grammar]
	LRPP
		Alessandro Warth, James R. Douglass, and Todd Millstein, Packrat parsers can support left recursion. Workshop on Partial Evaluation and Program Manipulation (PEPM '08), January 2008. Describes a general method of extending the PEG model to support left recursion.
		@[http://portal.acm.org/citation.cfm?id=1328408.1328424]
		@[http://www.tinlizzie.org/~awarth/papers/pepm08.pdf]

	Memos
		Ralph Becket and Zoltan Somogyi, DCGs + Memoing = Packrat Parsing: But is it worth it? Practical Aspects of Declarative Languages (PADL '08), January 2008. Explores implementation of packrat parsers in logic programming languages such as Prolog and Mercury.
		@[http://www.mercury.csse.unimelb.edu.au/information/papers/packrat.pdf]

	Skiena
		Steven Skiena, The Algorithm Design Manual, Springer, ISBN 978-1-848000-069-8.
		
	Stepanov
		Alexander Stepanov, Paul McJones, Elements of Programming, Addison-Wesley, ISBN 978-0-321-63537-2



format
	title	h1:prose
	sect	h2:prose
	sub1	h3:prose
	para	p:prose
	eg	pre:text
	refs	dl:format
	*	ul:format

prose
	:_	tt
	~_	em
	"_"	quote
	#_	linkID
	@[_]	link
	^[_]	linkName

style
	.undef { color:red; background-color:yellow; }
	.title, .author, .date { text-align: center; }
	.eg { background-color:#FFFDDA; border: thin solid #ECECEC; }
	dl { font-size:small; }
	dl dd p { margin: 0pt 0pt 10pt 0pt; }
	.format, .prose, .style { display:none; }



